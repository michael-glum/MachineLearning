{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Layer_Neural_Network_For_Handwritten_Digits.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michael-glum/MachineLearning/blob/main/3_Layer_Neural_Network_For_Handwritten_Digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the following code to download the MNIST dataset\n"
      ],
      "metadata": {
        "id": "AFDo_kFWtYgr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITMOkIvdnyde",
        "outputId": "4c849c4f-51f7-4f5a-ae4e-d05ce62fdf2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Download complete.\n",
            "Save complete.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from urllib import request\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "filename = [\n",
        "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
        "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
        "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
        "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
        "]\n",
        "\n",
        "def download_mnist():\n",
        "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "    for name in filename:\n",
        "        print(\"Downloading \"+name[1]+\"...\")\n",
        "        request.urlretrieve(base_url+name[1], name[1])\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "def save_mnist():\n",
        "    mnist = {}\n",
        "    for name in filename[:2]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
        "    for name in filename[-2:]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    with open(\"mnist.pkl\", 'wb') as f:\n",
        "        pickle.dump(mnist,f)\n",
        "    print(\"Save complete.\")\n",
        "\n",
        "def init():\n",
        "    download_mnist()\n",
        "    save_mnist()\n",
        "#    print ((load()[0]).shape)\n",
        "def load():\n",
        "    with open(\"mnist.pkl\",'rb') as f:\n",
        "        mnist = pickle.load(f)\n",
        "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numpy implementation\nThe load() function will return the training and test dataset"
      ],
      "metadata": {
        "id": "KnuEMkWdtXc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from urllib import request\n",
        "import gzip\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "def load():\n",
        "    with open(\"mnist.pkl\",'rb') as f:\n",
        "        mnist = pickle.load(f)\n",
        "\n",
        "        training_images, training_labels, testing_images, testing_labels = mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
        "        # Normalize the images\n",
        "        training_images.astype('float32')\n",
        "        testing_images.astype('float32')\n",
        "        training_images = training_images / 255\n",
        "        testing_images = testing_images / 255\n",
        "        return training_images, training_labels, testing_images, testing_labels\n",
        "\n",
        "\n",
        "TRimg,TRlab,TSimg,TSlab=load()\n",
        "print(len(TRimg),len(TRlab),len(TSimg),len(TSlab))\n",
        "print(len(TRimg[0]),len(TRlab),len(TSimg[0]),len(TSlab))\n",
        "\n",
        "arr_2d = np.reshape(TRimg[0], (28, 28))\n",
        "\n",
        "epochs = 10\n",
        "batchSize = 32 # or 128\n",
        "lr = 0.1 # or 0.01 or 0.001\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "h=200\n",
        "W=lr*np.random.randn(784, 200)\n",
        "b=np.zeros((1, h))\n",
        "h2=50\n",
        "W2=lr*np.random.randn(200,50)\n",
        "b2=np.zeros((1, h2))\n",
        "h3=10\n",
        "W3=lr*np.random.randn(50,10)\n",
        "b3=np.zeros((1, h3))\n",
        "\n",
        "step_size = batchSize\n",
        "reg = 1e-3\n",
        "\n",
        "for x in range(epochs):\n",
        "  batchStart = 0\n",
        "  i = 0\n",
        "  while i < 60000:\n",
        "    if 60000 - (i + batchSize) < 0:\n",
        "      step_size = 60000 - i\n",
        "      i += step_size\n",
        "    else:\n",
        "      i += batchSize\n",
        "\n",
        "    batch = TRimg[batchStart:i]\n",
        "    labels = TRlab[batchStart:i]\n",
        "    scores = np.empty([batchSize,h3])\n",
        "\n",
        "\n",
        "    # calculate hidden layers\n",
        "    hidden_layer1=np.maximum(0,np.dot(batch,W)+b)\n",
        "    hidden_layer2=np.maximum(0,np.dot(hidden_layer1,W2)+b2)\n",
        "    scores=np.dot(hidden_layer2,W3)+b3\n",
        "\n",
        "    # calculate output layer\n",
        "    exp_scores=np.exp(scores)\n",
        "    probs=exp_scores/np.sum(exp_scores,axis=1,keepdims=True)\n",
        "\n",
        "    num_examples = batch.shape[0]\n",
        "    correct_logprobs=-np.log(probs[range(num_examples), labels])\n",
        "    data_loss=np.sum(correct_logprobs)/num_examples\n",
        "    reg_loss=0.5*reg*np.sum(W*W)+0.5*reg*np.sum(W2*W2)+0.5*reg*np.sum(W3*W3)\n",
        "    loss=data_loss+reg_loss\n",
        "\n",
        "    if i%3000==0:\n",
        "      print(\"iteration %d: loss %f\" % (i,loss))\n",
        "\n",
        "    batchStart = i\n",
        "\n",
        "    dscores=probs\n",
        "    dscores[range(num_examples), labels]-=1\n",
        "    dscores/=num_examples\n",
        "\n",
        "    dw3=np.dot(hidden_layer2.T,dscores)\n",
        "    db3=np.sum(dscores, axis=0,keepdims=True)\n",
        "\n",
        "    dhidden2=np.dot(dscores,W3.T)\n",
        "    dhidden2[hidden_layer2 <=0]=0\n",
        "\n",
        "    dw2=np.dot(hidden_layer1.T,dhidden2)\n",
        "    db2=np.sum(dhidden2,axis=0,keepdims=True)\n",
        "\n",
        "    dhidden=np.dot(dhidden2,W2.T)\n",
        "    dhidden[hidden_layer1 <=0]=0\n",
        "\n",
        "    dw=np.dot(batch.T,dhidden)\n",
        "    db=np.sum(dhidden, axis=0,keepdims=True)\n",
        "\n",
        "    dw3+=reg*W3\n",
        "    dw2+=reg*W2\n",
        "    dw+=reg*W\n",
        "\n",
        "    W+=-lr*dw\n",
        "    b+=-lr*db\n",
        "    W2+=-lr*dw2\n",
        "    b2+=-lr*db2\n",
        "    W3+=-lr*dw3\n",
        "    b3+=-lr*db3\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoWNw2yFoOP3",
        "outputId": "39064d69-1cdd-423c-d4df-276a751628c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 60000 10000 10000\n",
            "784 60000 784 10000\n",
            "iteration 12000: loss 1.033194\n",
            "iteration 24000: loss 1.171413\n",
            "iteration 36000: loss 0.890956\n",
            "iteration 48000: loss 0.768000\n",
            "iteration 60000: loss 0.666515\n",
            "iteration 12000: loss 0.652162\n",
            "iteration 24000: loss 0.680489\n",
            "iteration 36000: loss 0.593643\n",
            "iteration 48000: loss 0.544788\n",
            "iteration 60000: loss 0.480006\n",
            "iteration 12000: loss 0.485317\n",
            "iteration 24000: loss 0.479842\n",
            "iteration 36000: loss 0.432525\n",
            "iteration 48000: loss 0.404388\n",
            "iteration 60000: loss 0.359523\n",
            "iteration 12000: loss 0.374334\n",
            "iteration 24000: loss 0.364174\n",
            "iteration 36000: loss 0.340760\n",
            "iteration 48000: loss 0.318734\n",
            "iteration 60000: loss 0.279363\n",
            "iteration 12000: loss 0.297520\n",
            "iteration 24000: loss 0.287435\n",
            "iteration 36000: loss 0.276574\n",
            "iteration 48000: loss 0.256604\n",
            "iteration 60000: loss 0.226388\n",
            "iteration 12000: loss 0.234374\n",
            "iteration 24000: loss 0.241939\n",
            "iteration 36000: loss 0.232846\n",
            "iteration 48000: loss 0.215933\n",
            "iteration 60000: loss 0.188207\n",
            "iteration 12000: loss 0.200495\n",
            "iteration 24000: loss 0.209444\n",
            "iteration 36000: loss 0.201922\n",
            "iteration 48000: loss 0.190147\n",
            "iteration 60000: loss 0.165232\n",
            "iteration 12000: loss 0.175467\n",
            "iteration 24000: loss 0.181829\n",
            "iteration 36000: loss 0.182162\n",
            "iteration 48000: loss 0.167384\n",
            "iteration 60000: loss 0.144930\n",
            "iteration 12000: loss 0.159806\n",
            "iteration 24000: loss 0.169556\n",
            "iteration 36000: loss 0.163912\n",
            "iteration 48000: loss 0.158473\n",
            "iteration 60000: loss 0.133037\n",
            "iteration 12000: loss 0.144438\n",
            "iteration 24000: loss 0.157154\n",
            "iteration 36000: loss 0.154064\n",
            "iteration 48000: loss 0.146438\n",
            "iteration 60000: loss 0.124390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer1=np.maximum(0,np.dot(TSimg,W)+b)\n",
        "hidden_layer2=np.maximum(0,np.dot(hidden_layer1,W2)+b2)\n",
        "scores=np.dot(hidden_layer2,W3)+b3\n",
        "predicted_class=np.argmax(scores,axis=1)\n",
        "print(\"training accuracy: %.2f\" % (np.mean(predicted_class==TSlab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9tadKKxD6fQ",
        "outputId": "8bb6dd6c-f168-47ab-fbf2-675b54fcf295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you are interested in what the data looks like. You can run the following code to see."
      ],
      "metadata": {
        "id": "j8F879VhtwVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "showInd=66\n",
        "Column=np.reshape(TRimg[showInd], (784, 1))\n",
        "arr_2d = np.reshape(TRimg[showInd], (28, 28))\n",
        "arr_2dT=np.rot90(arr_2d)\n",
        "arr_2dTA=np.rot90(arr_2dT)\n",
        "Final=np.fliplr(arr_2dTA)\n",
        "#arr_2dTA=np.transpose(arr_2dT)\n",
        "#arr_2dTB=np.transpose(arr_2dTA)\n",
        "plt.pcolor(Final)\n",
        "print(TRlab[showInd])\n",
        "plt.show()\n",
        "\n",
        "plt.pcolor(Column)\n",
        "plt.show()\n",
        "\n",
        "#pcm = plt.pcolormesh(arr_2d,cmap='RdBu_r')\n",
        "#plt.colorbar(pcm)\n",
        "#plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "x1JRTuxiwqVa",
        "outputId": "ae99fc38-160e-409c-87ef-a25d3a284955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [For dataset loading in Pytorch, you can refer to the following example code](https://github.com/pytorch/examples/blob/master/mnist/main.py)\n",
        "\n",
        "https://github.com/pytorch/examples/blob/master/mnist/main.py \n"
      ],
      "metadata": {
        "id": "EYoRM5S3A8y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "training_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "  def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = read_image(img_path)\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(image)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    sample = {\"image\": image, \"label\": label}\n",
        "    return sample\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 200),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(200, 50),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(50, 10),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "batch = 128\n",
        "learning_rate = 0.1 # or 0.01 or 0.001\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Compute prediction and loss\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100==0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "  print(\"Done!\")"
      ],
      "metadata": {
        "id": "7je10p5XA8dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61bc3798-a8dc-46ea-e68d-5e0eb3ce033b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=200, out_features=50, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n",
            "Predicted class: tensor([0])\n",
            "Epoch 1\n",
            "-------------------------\n",
            "loss: 2.303690 [    0/60000]\n",
            "loss: 0.831231 [ 3200/60000]\n",
            "loss: 0.487881 [ 6400/60000]\n",
            "loss: 0.360394 [ 9600/60000]\n",
            "loss: 0.451005 [12800/60000]\n",
            "loss: 0.196018 [16000/60000]\n",
            "loss: 0.327802 [19200/60000]\n",
            "loss: 0.197296 [22400/60000]\n",
            "loss: 0.131264 [25600/60000]\n",
            "loss: 0.287865 [28800/60000]\n",
            "loss: 0.212819 [32000/60000]\n",
            "loss: 0.388266 [35200/60000]\n",
            "loss: 0.201947 [38400/60000]\n",
            "loss: 0.186875 [41600/60000]\n",
            "loss: 0.109582 [44800/60000]\n",
            "loss: 0.068072 [48000/60000]\n",
            "loss: 0.235175 [51200/60000]\n",
            "loss: 0.152902 [54400/60000]\n",
            "loss: 0.057095 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.005261 \n",
            "\n",
            "Done!\n",
            "Epoch 2\n",
            "-------------------------\n",
            "loss: 0.234630 [    0/60000]\n",
            "loss: 0.248677 [ 3200/60000]\n",
            "loss: 0.532252 [ 6400/60000]\n",
            "loss: 0.238085 [ 9600/60000]\n",
            "loss: 0.122052 [12800/60000]\n",
            "loss: 0.083338 [16000/60000]\n",
            "loss: 0.118976 [19200/60000]\n",
            "loss: 0.217692 [22400/60000]\n",
            "loss: 0.088928 [25600/60000]\n",
            "loss: 0.030582 [28800/60000]\n",
            "loss: 0.131684 [32000/60000]\n",
            "loss: 0.160472 [35200/60000]\n",
            "loss: 0.145865 [38400/60000]\n",
            "loss: 0.224455 [41600/60000]\n",
            "loss: 0.132732 [44800/60000]\n",
            "loss: 0.394403 [48000/60000]\n",
            "loss: 0.274712 [51200/60000]\n",
            "loss: 0.045932 [54400/60000]\n",
            "loss: 0.221753 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.003262 \n",
            "\n",
            "Done!\n",
            "Epoch 3\n",
            "-------------------------\n",
            "loss: 0.171930 [    0/60000]\n",
            "loss: 0.105786 [ 3200/60000]\n",
            "loss: 0.042570 [ 6400/60000]\n",
            "loss: 0.035084 [ 9600/60000]\n",
            "loss: 0.013031 [12800/60000]\n",
            "loss: 0.028832 [16000/60000]\n",
            "loss: 0.088005 [19200/60000]\n",
            "loss: 0.060837 [22400/60000]\n",
            "loss: 0.062526 [25600/60000]\n",
            "loss: 0.041045 [28800/60000]\n",
            "loss: 0.020972 [32000/60000]\n",
            "loss: 0.025040 [35200/60000]\n",
            "loss: 0.087044 [38400/60000]\n",
            "loss: 0.053129 [41600/60000]\n",
            "loss: 0.108624 [44800/60000]\n",
            "loss: 0.131166 [48000/60000]\n",
            "loss: 0.063264 [51200/60000]\n",
            "loss: 0.046286 [54400/60000]\n",
            "loss: 0.020657 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.002606 \n",
            "\n",
            "Done!\n",
            "Epoch 4\n",
            "-------------------------\n",
            "loss: 0.052155 [    0/60000]\n",
            "loss: 0.030944 [ 3200/60000]\n",
            "loss: 0.118636 [ 6400/60000]\n",
            "loss: 0.197598 [ 9600/60000]\n",
            "loss: 0.022410 [12800/60000]\n",
            "loss: 0.074443 [16000/60000]\n",
            "loss: 0.093556 [19200/60000]\n",
            "loss: 0.016611 [22400/60000]\n",
            "loss: 0.127192 [25600/60000]\n",
            "loss: 0.119121 [28800/60000]\n",
            "loss: 0.070070 [32000/60000]\n",
            "loss: 0.116851 [35200/60000]\n",
            "loss: 0.163925 [38400/60000]\n",
            "loss: 0.027137 [41600/60000]\n",
            "loss: 0.184184 [44800/60000]\n",
            "loss: 0.019991 [48000/60000]\n",
            "loss: 0.114391 [51200/60000]\n",
            "loss: 0.018594 [54400/60000]\n",
            "loss: 0.172719 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.002257 \n",
            "\n",
            "Done!\n",
            "Epoch 5\n",
            "-------------------------\n",
            "loss: 0.003226 [    0/60000]\n",
            "loss: 0.169135 [ 3200/60000]\n",
            "loss: 0.004818 [ 6400/60000]\n",
            "loss: 0.209535 [ 9600/60000]\n",
            "loss: 0.004553 [12800/60000]\n",
            "loss: 0.009695 [16000/60000]\n",
            "loss: 0.012318 [19200/60000]\n",
            "loss: 0.146524 [22400/60000]\n",
            "loss: 0.005850 [25600/60000]\n",
            "loss: 0.003438 [28800/60000]\n",
            "loss: 0.031828 [32000/60000]\n",
            "loss: 0.012410 [35200/60000]\n",
            "loss: 0.125934 [38400/60000]\n",
            "loss: 0.064545 [41600/60000]\n",
            "loss: 0.002353 [44800/60000]\n",
            "loss: 0.016849 [48000/60000]\n",
            "loss: 0.046052 [51200/60000]\n",
            "loss: 0.003509 [54400/60000]\n",
            "loss: 0.027059 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.002621 \n",
            "\n",
            "Done!\n",
            "Epoch 6\n",
            "-------------------------\n",
            "loss: 0.071462 [    0/60000]\n",
            "loss: 0.011908 [ 3200/60000]\n",
            "loss: 0.038717 [ 6400/60000]\n",
            "loss: 0.011123 [ 9600/60000]\n",
            "loss: 0.035994 [12800/60000]\n",
            "loss: 0.027676 [16000/60000]\n",
            "loss: 0.405984 [19200/60000]\n",
            "loss: 0.023619 [22400/60000]\n",
            "loss: 0.010993 [25600/60000]\n",
            "loss: 0.002885 [28800/60000]\n",
            "loss: 0.008268 [32000/60000]\n",
            "loss: 0.057053 [35200/60000]\n",
            "loss: 0.067859 [38400/60000]\n",
            "loss: 0.017816 [41600/60000]\n",
            "loss: 0.030048 [44800/60000]\n",
            "loss: 0.003218 [48000/60000]\n",
            "loss: 0.035234 [51200/60000]\n",
            "loss: 0.130630 [54400/60000]\n",
            "loss: 0.382700 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.002247 \n",
            "\n",
            "Done!\n",
            "Epoch 7\n",
            "-------------------------\n",
            "loss: 0.009153 [    0/60000]\n",
            "loss: 0.016649 [ 3200/60000]\n",
            "loss: 0.016206 [ 6400/60000]\n",
            "loss: 0.030720 [ 9600/60000]\n",
            "loss: 0.079230 [12800/60000]\n",
            "loss: 0.119009 [16000/60000]\n",
            "loss: 0.002324 [19200/60000]\n",
            "loss: 0.000889 [22400/60000]\n",
            "loss: 0.034165 [25600/60000]\n",
            "loss: 0.000400 [28800/60000]\n",
            "loss: 0.007962 [32000/60000]\n",
            "loss: 0.069561 [35200/60000]\n",
            "loss: 0.017451 [38400/60000]\n",
            "loss: 0.016377 [41600/60000]\n",
            "loss: 0.015694 [44800/60000]\n",
            "loss: 0.029194 [48000/60000]\n",
            "loss: 0.018131 [51200/60000]\n",
            "loss: 0.028333 [54400/60000]\n",
            "loss: 0.007531 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.002199 \n",
            "\n",
            "Done!\n",
            "Epoch 8\n",
            "-------------------------\n",
            "loss: 0.002478 [    0/60000]\n",
            "loss: 0.032738 [ 3200/60000]\n",
            "loss: 0.001593 [ 6400/60000]\n",
            "loss: 0.008564 [ 9600/60000]\n",
            "loss: 0.000652 [12800/60000]\n",
            "loss: 0.006892 [16000/60000]\n",
            "loss: 0.006402 [19200/60000]\n",
            "loss: 0.008395 [22400/60000]\n",
            "loss: 0.001596 [25600/60000]\n",
            "loss: 0.031811 [28800/60000]\n",
            "loss: 0.003683 [32000/60000]\n",
            "loss: 0.008722 [35200/60000]\n",
            "loss: 0.054424 [38400/60000]\n",
            "loss: 0.033958 [41600/60000]\n",
            "loss: 0.068413 [44800/60000]\n",
            "loss: 0.014431 [48000/60000]\n",
            "loss: 0.000940 [51200/60000]\n",
            "loss: 0.037379 [54400/60000]\n",
            "loss: 0.004850 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.002189 \n",
            "\n",
            "Done!\n",
            "Epoch 9\n",
            "-------------------------\n",
            "loss: 0.003714 [    0/60000]\n",
            "loss: 0.000982 [ 3200/60000]\n",
            "loss: 0.013366 [ 6400/60000]\n",
            "loss: 0.013995 [ 9600/60000]\n",
            "loss: 0.012453 [12800/60000]\n",
            "loss: 0.006719 [16000/60000]\n",
            "loss: 0.018107 [19200/60000]\n",
            "loss: 0.027003 [22400/60000]\n",
            "loss: 0.002539 [25600/60000]\n",
            "loss: 0.041709 [28800/60000]\n",
            "loss: 0.006267 [32000/60000]\n",
            "loss: 0.000533 [35200/60000]\n",
            "loss: 0.006281 [38400/60000]\n",
            "loss: 0.004062 [41600/60000]\n",
            "loss: 0.009587 [44800/60000]\n",
            "loss: 0.000648 [48000/60000]\n",
            "loss: 0.027873 [51200/60000]\n",
            "loss: 0.045178 [54400/60000]\n",
            "loss: 0.020023 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.002265 \n",
            "\n",
            "Done!\n",
            "Epoch 10\n",
            "-------------------------\n",
            "loss: 0.011922 [    0/60000]\n",
            "loss: 0.015723 [ 3200/60000]\n",
            "loss: 0.077242 [ 6400/60000]\n",
            "loss: 0.006510 [ 9600/60000]\n",
            "loss: 0.000826 [12800/60000]\n",
            "loss: 0.008219 [16000/60000]\n",
            "loss: 0.009322 [19200/60000]\n",
            "loss: 0.191527 [22400/60000]\n",
            "loss: 0.000403 [25600/60000]\n",
            "loss: 0.005165 [28800/60000]\n",
            "loss: 0.004626 [32000/60000]\n",
            "loss: 0.001103 [35200/60000]\n",
            "loss: 0.005757 [38400/60000]\n",
            "loss: 0.002076 [41600/60000]\n",
            "loss: 0.008213 [44800/60000]\n",
            "loss: 0.000458 [48000/60000]\n",
            "loss: 0.057264 [51200/60000]\n",
            "loss: 0.034551 [54400/60000]\n",
            "loss: 0.020096 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.002609 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}
